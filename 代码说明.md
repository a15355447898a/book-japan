# 小説家になろう 小说爬虫项目

## web scraper版本

### 爬取方式

依旧是使用浏览器的web scraper插件爬取数据(主要是懒得写代码)

### 数据清洗

web scraper插件爬取的数据顺序是混乱的,所以需要用excel进行数据清洗

原始数据如下:

| web-scraper-order    | web-scraper-start-url | link   | link-href   | text |
| -------------------- | --------------------- | ------ | ----------- | ---- |
| 不知道干什么用的东西 | 主页面URL             | 标题名 | 正文部分URL | 正文 |

更改成下面这个样子:

| web-scraper-order    | web-scraper-start-url | link   | link-href   | text | number                                                       |
| -------------------- | --------------------- | ------ | ----------- | ---- | ------------------------------------------------------------ |
| 不知道干什么用的东西 | 主页面URL             | 标题名 | 正文部分URL | 正文 | =VALUE(RegexString(D2,"\d+",0,2))<br />(得到正文部分URL中最后的数字,用于排序) |

### 数据导入mathematica,拼接,导出

```mathematica
排序好的数据 = Import@"/home/sunwenbin/下载/日本へようこそエルフさん(排序过).csv"
(*排序好的数据[[All,5]][[2]](*正文*)*)
(*排序好的数据[[All,3]][[2]](*章节名称*)*)
拼接正文标题与正文 = Table["### " <> 排序好的数据[[All, 3]][[i]] <> "\n" <>排序好的数据[[All, 5]][[i]], {i, 2, Length@排序好的数据[[All, 3]] - 1}];
Export["小说.txt", StringReplace[Join@拼接正文标题与正文, "\n" -> "\n\n"]]
```

> 注意,只能处理一级标题,剩下的要手动添加正在想如何写代码完成所有事情

## mathematica版本

根据目录部分得到章节结构与正文所在的URL

```mathematica
目录原文 = StringCases[
ToString@URLRead["https://ncode.syosetu.com/n9354du/"(*小说的网址*), "Body"]
,RegularExpression["<div class=\"chapter_title\">(.*)</div>|<dd class=\"subtitle\">\\n(.*)\n</dd>"] -> {"$1", "$2"}];
目录处理[{x_, y_}] := 
  If[x == ""
  ,(*小章节*){StringCases[y
  ,RegularExpression@"\">(.*)</a>" -> "$1"][[1]]
  ,"https://ncode.syosetu.com" <>StringCases[y
  ,RegularExpression@"<a href=\"(.*)\">" -> "$1"][[1]]},(*大章节*){x}];
目录处理 /@ 目录原文
```

一步到位,得到正文,拼接导出(没有加上书名)

```mathematica
目录原文=StringCases[ToString@URLRead["https://ncode.syosetu.com/n9354du/"(*小说的网址*),"Body"]
,RegularExpression["<div class=\"chapter_title\">(.*)</div>|<dd class=\"subtitle\">\n(.*)\n</dd>"]->{"$1","$2"}];
目录处理[{x_,y_}]:=
If[x==""
,(*小章节*){
(*标题*)"### "<>StringCases[y,RegularExpression@"\">(.*)</a>"->"$1"][[1]]<>"\n\n"
,(*正文*)
StringReplace[
StringCases[
ToString@URLRead["https://ncode.syosetu.com"<>StringCases[y,RegularExpression@"<a href=\"(.*)\">"->"$1"][[1]],"Body"]
,RegularExpression@"<div id=\"novel_honbun\" class=\"novel_view\">((?:.|\n)*?)</div>"->"$1"]
,"<"~~Except[">"]..~~">"->""]
}
,(*大章节*){"## "<>x<>"\n\n"}];
原文列表=目录处理/@目录原文;
原文=Flatten@原文列表//Join;
Export["原文.txt",原文]
```

把文件名命名为小说名,自动重命名为md

```mathematica
书的链接 = "https://ncode.syosetu.com/n9354du/";
目录源代码 = ToString@URLRead[书的链接(*小说的网址*), "Body"];
书名 = StringCases[目录源代码, 
    RegularExpression@"<p class=\"novel_title\">(.*)</p>" -> 
     "$1"][[1]];
目录原文 = StringCases[目录源代码
   , RegularExpression[
     "<div class=\"chapter_title\">(.*)</div>|<dd class=\"subtitle\">\n(.*)\n</dd>"] -> {"$1", "$2"}];
目录处理[{x_, y_}] :=
  If[x == ""
   ,(*小章节*){
    (*标题*)"### " <> 
     StringCases[y, RegularExpression@"\">(.*)</a>" -> "$1"][[1]] <> 
     "\n\n"
    ,(*正文*)
    StringReplace[
     StringCases[
      ToString@
       URLRead["https://ncode.syosetu.com" <> 
         StringCases[y, 
           RegularExpression@"<a href=\"(.*)\">" -> "$1"][[1]], "Body"]
      , RegularExpression@
        "<div id=\"novel_honbun\" class=\"novel_view\">((?:.|\n\)*?)</div>" -> "$1"]
     , "<" ~~ Except[">"] .. ~~ ">" -> ""]
    }
   ,(*大章节*){"## " <> x <> "\n\n"}];
原文列表 = Prepend[目录处理 /@ 目录原文, {"# " <> 书名 <> "\n\n"}];
原文 = Flatten@原文列表 // Join;
原文件名 = Export[书名 <> ".txt", 原文];
RenameFile[原文件名, 书名 <> ".md"];
```

